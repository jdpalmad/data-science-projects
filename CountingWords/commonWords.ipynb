{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Tools for text processing**\n",
    "\n",
    "So we are here to answer a simple question.\n",
    "*What is the most common word in Moby Dick by Herman Melville?* \n",
    "\n",
    "Obviusly we will first get the data, it will be scrapped from **Project Gutenberg** using the *requests* package and *BeautifulSoup*.\n",
    "\n",
    "Then, naturally, we will check de dataset and look for the most frequent words and their occurencias after carefuly prepare the data if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing requests, BeautifulSoup, nltk, and Counter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported the pertinent packages to build the data pipeline, let's get the data.\n",
    "\n",
    "## **2. Request Moby Dick**\n",
    "\n",
    "As mentioneed before, we can scrap the novel freely from the **Project Gutenberg** website.\n",
    "Let's fetch the HTML file using the *requests* package to make a *GET* request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "\n",
      "<!DOCTYPE html\n",
      "   PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n",
      "   \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\" >\n",
      "\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
      "  <head>\n",
      "    <title>\n",
      "      Moby Dick; Or the Whale, by Herman Melville\n",
      "    </title>\n",
      "    <style type=\"text/css\" xml:space=\"preserve\">\n",
      "\n",
      "    body { background:#faebd0; color:black; margin-left:15%; margin-right:15%; text-align:justify }\n",
      "    P { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\n",
      "    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\n",
      "    hr  { width: 50%; text-align: center;}\n",
      "    .foot { margin-left: 20%; margin-right: 20%; text-align: justify; text-indent: -3em; font-size: 90%; }\n",
      "    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\n",
      "    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\n",
      "    .toc       { margin-left: 10%; margin-bottom: .75em;}\n",
      "    .toc2      { margin-left: 20%;}\n",
      "    div.fig    { display:block; margin:0 auto; text-align:center; }\n",
      "    div.middle { margin-left: 20%; margin-right: 20%; text-align: justify; }\n",
      "    .figleft   {float: left; margin-left: 0%; margin-right: 1%;}\n",
      "    .figright  {float: right; margin-right: 0%; margin-left: 1%;}\n",
      "    .pagenum   {display:inline; font-size: 70%; font-style:normal;\n",
      "               margin: 0; padding: 0; position: absolute; right: 1%;\n",
      "               text-align: right;}\n",
      "    pre        { font-family: times new roman; font-size: 100%; margin-left: 10%;}\n",
      "\n",
      "    table      {margin-left: 10%;}\n",
      "\n",
      "a:link {color:blue;\n",
      "\t\ttext-decoration:none}\n",
      "link {color:blue;\n",
      "\t\ttext-decoration:none}\n",
      "a:visited {color:blue;\n",
      "\t\ttext-decoration:none}\n",
      "a:hover {color:red}\n",
      "\n",
      "</style>\n",
      "  </head>\n",
      "  <body>\n",
      "<pre xml:space=\"preserve\">\n",
      "\n",
      "The Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville\n",
      "\n",
      "This eBook is for the use of anyone anywh\n"
     ]
    }
   ],
   "source": [
    "# Getting the Moby Dick HTML \n",
    "r = requests.get('https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\n",
    "\n",
    "# Setting the correct text encoding of the HTML page\n",
    "r.encoding = 'utf-8'\n",
    "\n",
    "# Extracting the HTML from the request object\n",
    "html = r.text\n",
    "\n",
    "# Printing the first 2000 characters in html\n",
    "print(html[0:2000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly now we have to start reading the book and scraping the data.\n",
    "\n",
    "## **3. Get the text from the HTML**\n",
    "\n",
    "Note that we got an HTML file, but we actually need the text inside of it, to get the text we will use BeautifulSoup to parse the HTML and extract the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rzlaqk/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " William Comstock. Another Version of the whale-ship Globe\n",
      "        narrative.\n",
      "      \n",
      "\n",
      "        “The voyages of the Dutch and English to the Northern Ocean, in order,\n",
      "        if possible, to discover a passage through it to India, though they\n",
      "        failed of their main object, laid-open the haunts of the whale.” —McCulloch’s\n",
      "        Commercial Dictionary.\n",
      "      \n",
      "\n",
      "        “These things are reciprocal; the ball rebounds, only to bound forward\n",
      "        again; for now in laying open the haunts of the whale, the whalemen seem\n",
      "        to have indirectly hit upon new clews to that same mystic North-West\n",
      "        Passage.” —From “Something” unpublished.\n",
      "      \n",
      "\n",
      "        “It is impossible to meet a whale-ship on the ocean without being struck\n",
      "        by her near appearance. The vessel under short sail, with look-outs at\n",
      "        the mast-heads, eagerly scanning the wide expanse around them, has a\n",
      "        totally different air from those engaged in regular voyage.” —Currents\n",
      "        and Whaling. U.S. Ex. Ex.\n",
      "      \n",
      "\n",
      "        “Pedestrians in the vicinity of London and elsewhere may recollect\n",
      "        having seen large curved bones set upright in the earth, either to form\n",
      "        arches over gateways, or entrances to alcoves, and they may perhaps have\n",
      "        been told that these were the ribs of whales.” —Tales of a Whale\n",
      "        Voyager to the Arctic Ocean.\n",
      "      \n",
      "\n",
      "        “It was not till the boats returned from the pursuit of these whales,\n",
      "        that the whites saw their ship in bloody possession of the savages\n",
      "        enrolled among the crew.” —Newspaper Account of the Taking and\n",
      "        Retaking of the Whale-Ship Hobomack.\n",
      "      \n",
      "\n",
      "        “It is generally well known that out of the crews of Whaling vessels\n",
      "        (American) few ever return in the ships on board of which they\n",
      "        departed.” —Cruise in a Whale Boat.\n",
      "      \n",
      "\n",
      "        “Suddenly a mighty mass emerged from the water, and shot up\n",
      "        perpendicularly into the air. It was the whale.” —Miriam Coffin or\n",
      "        the Whale Fisherman.\n",
      "      \n",
      "\n",
      "        “The Whale is harpooned to be sure; but bethink you, how you would\n",
      "        manage a powerful unbroken colt, with the mere appliance of a rope tied\n",
      "        to the root of his tail.” —A Chapter on Whaling in Ribs and\n",
      "        Trucks.\n",
      "      \n",
      "\n",
      "        “On one occasion I saw two of these monsters (whales) probably male and\n",
      "        female, slowly swimming, one after the other, within less than a stone’s\n",
      "        throw of the shore” (Terra Del Fuego), “over which the beech tree\n",
      "        extended its branches.” —Darwin’s Voyage of a Naturalist.\n",
      "      \n",
      "\n",
      "        “‘Stern all!’ exclaimed the mate, as upon turning his head, he saw the\n",
      "        distended jaws of a large Sperm Whale close to the head of the boat,\n",
      "        threatening it with instant destruction;—‘Stern all, for your\n",
      "        lives!’” —Wharton the Whale Killer.\n",
      "      \n",
      "\n",
      "        “So be cheery, my lads, let your hearts never fail, While the bold\n",
      "        harpooneer is striking the whale!” —Nantucket Song.\n",
      "      \n",
      "\n",
      "     “Oh, the rare old Whale, mid storm and gale\n",
      "     In his ocean home will be\n",
      "     A giant in might, where might is right,\n",
      "     And King of the boundless sea.”\n",
      "      —Whale Song.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      CHAPTER 1. Loomings.\n",
      "    \n",
      "\n",
      "      Call me Ishmael. Some years ago—never mind how long precisely—having\n",
      "      little or no money in my purse, and nothing particular to interest me on\n",
      "      shore, I thought I would sail about a little and see the watery part of\n",
      "      the world. It is a way I have of driving off the spleen and regulating the\n",
      "      circulation. Whenever I find myself growing grim about the mouth; whenever\n",
      "      it is a damp, drizzly November in my soul; whenever I find myself\n",
      "      involuntarily pausing before coffin warehouses, and bringing up the rear\n",
      "      of every funeral I meet; and especially whenever my hypos get such an\n",
      "      upper hand of me, that it requires a strong moral principle to preven\n"
     ]
    }
   ],
   "source": [
    "# Creating a BeautifulSoup object from the HTML\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "# Getting the text out of the soup\n",
    "text = soup.get_text()\n",
    "\n",
    "# Printing out text between characters 28000 and 32000\n",
    "print(text[28000:32000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Extracting the words**\n",
    "\n",
    "Observe that still we have unwanted characters, like the punctuation marks, credits and other things that are not actually our scoped words, but to be fair its not probably a big deal looking at the size of the book. So, lets count the words.\n",
    "\n",
    "To do this is pertinent to use *NTLK* to start tokenizing the words and remove unwanted characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Moby',\n",
       " 'Dick',\n",
       " 'Or',\n",
       " 'the',\n",
       " 'Whale',\n",
       " 'by',\n",
       " 'Herman',\n",
       " 'Melville',\n",
       " 'The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'Moby',\n",
       " 'Dick',\n",
       " 'or',\n",
       " 'The',\n",
       " 'Whale',\n",
       " 'by',\n",
       " 'Herman',\n",
       " 'Melville',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a tokenizer\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "# Tokenizing the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Printing out the first 25 words / tokens \n",
    "tokens[0:25]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Make the words lowercase**\n",
    "\n",
    "Note to avoid counting \"Or\" and \"or\" as different words, we will make all the words lowercase.\n",
    "So we are going to save the lower tkenized words in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville', 'the', 'project', 'gutenberg', 'ebook', 'of', 'moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville', 'this', 'ebook', 'is', 'for']\n"
     ]
    }
   ],
   "source": [
    "# Create a list called words containing all tokens transformed to lower-case\n",
    "words = []\n",
    "for token in tokens:\n",
    "    words.append(token.lower())\n",
    "\n",
    "# Printing out the first 25 words / tokens \n",
    "print(words[0:25])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Load in stop words**\n",
    "\n",
    "There are some words that are not relevant now, like \"The\", \"of\", \"a\", etc...\n",
    "These words are known like stop words and again we will use *ntlk* to remove this words from our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rzlaqk/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves']\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Getting the English stop words from nltk\n",
    "nltk.download('stopwords')\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Printing out the first eight stop words\n",
    "print(sw[:8])\n",
    "print(sw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Remove stop words in Moby Dick**\n",
    "\n",
    "Now that we have the stop words, we can make our final list without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moby', 'dick', 'whale', 'herman', 'melville', 'project', 'gutenberg', 'ebook', 'moby', 'dick', 'whale', 'herman', 'melville', 'ebook', 'use']\n"
     ]
    }
   ],
   "source": [
    "# A new list to hold Moby Dick with No Stop words\n",
    "words_ns = []\n",
    "\n",
    "# Appending to words_ns all words that are in words but not in sw\n",
    "for word in words:\n",
    "    if word not in sw:\n",
    "        words_ns.append(word)\n",
    "\n",
    "# Printing the first 15 words_ns to check that stop words are gone\n",
    "print(words_ns[:15])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Answering the question**\n",
    "\n",
    "Now that we have the list, there is multiple ways to find the ocurrences of each word, but we will use the *Counter* function from *collections* package to count the words this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('whale', 1246), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Counter object from our processed list of words\n",
    "count = Counter(words_ns)\n",
    "\n",
    "# Store 10 most common words and their counts as top_ten\n",
    "top_ten = count.most_common(10)\n",
    "\n",
    "# Print the top ten words and their counts\n",
    "print(top_ten)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. The most common word**\n",
    "\n",
    "Finally, we can see that the most common word is \"whale\" with 1246 ocurrences followed by \"one\" with 925 ocurrences and so on..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *This was collected and solved by jdpalmad. Suggestions are found at Datacamp and the Book is scrapped from Project Gutenberg repository.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
